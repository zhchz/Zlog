<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0"><link rel="icon" type="image/png" sizes="32x32" href="/Zlog/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/Zlog/images/favicon.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="google-site-verification" content="JWKXKBgMA-PwT6UVFtoe_WnlTH8jJAuNK1XVhHY_2mM"><link rel="stylesheet" href="/Zlog/css/main.css"><link rel="stylesheet" href="/Zlog/lib/font-awesome/css/all.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"zrzfh.github.io",root:"/Zlog/",scheme:"Mist",version:"7.8.0",exturl:"fasle",sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!0,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!0,pangu:!0,comments:{style:"tabs",active:"gitalk",storage:!0,lazyload:!0,nav:null,activeClass:"gitalk"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="引言 不确定性可以说是在用数学工具对世界进行建模和认知的时候不可回避的问题，也是机器学习中的一个最基本的概念。于是，我们需要对这种不确定性进行量化和计算，而概率论能够为此提供一个合理的框架。所以，概率论是机器学习中最重要的基础理论之一，这也是本文要详细梳理概率论的基础知识的出发点。如果再辅之以其他工具如决策论、线性代数、微积分等，还能够在让我们构建世界观的同时，有切实可用的方法论。"><meta property="og:type" content="article"><meta property="og:title" content="机器学习基础（一）概率论"><meta property="og:url" content="https://zrzfh.github.io/Zlog/probability/index.html"><meta property="og:site_name" content="Zlog"><meta property="og:description" content="引言 不确定性可以说是在用数学工具对世界进行建模和认知的时候不可回避的问题，也是机器学习中的一个最基本的概念。于是，我们需要对这种不确定性进行量化和计算，而概率论能够为此提供一个合理的框架。所以，概率论是机器学习中最重要的基础理论之一，这也是本文要详细梳理概率论的基础知识的出发点。如果再辅之以其他工具如决策论、线性代数、微积分等，还能够在让我们构建世界观的同时，有切实可用的方法论。"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2023-04-03T08:06:15.000Z"><meta property="article:modified_time" content="2023-04-17T01:10:29.267Z"><meta property="article:author" content="zrzfh"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="不确定性"><meta property="article:tag" content="模式识别"><meta property="article:tag" content="概率论"><meta name="twitter:card" content="summary"><link rel="canonical" href="https://zrzfh.github.io/Zlog/probability/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>机器学习基础（一）概率论 | Zlog</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><link rel="alternate" href="/Zlog/atom.xml" title="Zlog" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/Zlog/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">Zlog</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/Zlog/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/Zlog/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/Zlog/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/Zlog/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-about"><a href="/Zlog/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><a role="button" class="book-mark-link book-mark-link-fixed"></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://zrzfh.github.io/Zlog/probability/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/Zlog/images/avatar.jpg"><meta itemprop="name" content="zrzfh"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Zlog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">机器学习基础（一）概率论</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2023-04-03 16:06:15" itemprop="dateCreated datePublished" datetime="2023-04-03T16:06:15+08:00">2023-04-03</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2023-04-17 09:10:29" itemprop="dateModified" datetime="2023-04-17T09:10:29+08:00">2023-04-17</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/Zlog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a> </span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>4.8k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>17 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="引言">引言</h1><p>不确定性可以说是在用数学工具对世界进行建模和认知的时候不可回避的问题，也是机器学习中的一个最基本的概念。于是，我们需要对这种不确定性进行量化和计算，而概率论能够为此提供一个合理的框架。所以，概率论是机器学习中最重要的基础理论之一，这也是本文要详细梳理概率论的基础知识的出发点。如果再辅之以其他工具如决策论、线性代数、微积分等，还能够在让我们构建世界观的同时，有切实可用的方法论。</p><span id="more"></span><p>本文会尽可能全面的列出机器学习中涉及到概率论的重要的基础的公理、定理、推理、定义、算法等，给出重要的公式、流程，但不会涉及证明和详细推导过程，作为梳理和备用，适用于入门和需要快速了解相关原理的情形，渴望更深层次的理解和探讨，建议系统的阅读相关资料书籍。</p><h1 id="随机变量与概率">随机变量与概率</h1><p>首先，我们简单的回顾一下随机事件、随机变量、概率等基本的概念。</p><h2 id="随机事件和随机变量">随机事件和随机变量</h2><p>对于不总是出现同一个结果的现象，我们称之为 <strong>随机现象 </strong>，随机现象所有的可能结果组成了<strong> 样本空间 </strong>，而样本空间中的一个或者几个点集被称为<strong> 随机事件</strong>，通常使用 <span class="math inline">\(A, B, C, ...\)</span> 等大写字母表示。</p><p>于是，用于表示随机事件的结果的数学变量，被称作 <strong>随机变量</strong>，在一般数学表达上使用 <span class="math inline">\(X, Y, Z\)</span> 这样的大写字母。随机变量可以用来表示随机事件或者随机事件的组合，于是涉及到了随机事件之间的关系和运算法则。</p><p>总的来说，随机事件的关系包括：<strong>包含关系 </strong>，<strong> 相等关系 </strong>，<strong> 互不相容</strong>。包含关系是指，事件 <span class="math inline">\(A\)</span> 是事件 <span class="math inline">\(B\)</span> 的子集，<span class="math inline">\(A\)</span> 的发生必然导致 <span class="math inline">\(B\)</span> 的发生，用 <span class="math inline">\(A \subset B\)</span> 表示。相等关系是指，事件 <span class="math inline">\(A\)</span> 和事件 <span class="math inline">\(B\)</span> 相互包含，用 <span class="math inline">\(A = B\)</span> 表示 。互不相容表示，事件 <span class="math inline">\(A\)</span> 和事件 <span class="math inline">\(B\)</span> 没有交集，<span class="math inline">\(A\)</span> 和 <span class="math inline">\(B\)</span> 不可能同时发生，用 <span class="math inline">\(A \cap B = \varnothing\)</span> 表示。</p><p>随机事件的运算法则包含 <strong>并</strong>，<strong>交 </strong>，<strong> 差</strong>，<strong>余</strong>。并：事件 <span class="math inline">\(A\)</span> 或者事件 <span class="math inline">\(B\)</span> 至少有一个发生，用 <span class="math inline">\(A \cup B\)</span> 表示；交：事件 <span class="math inline">\(A\)</span> 和事件 <span class="math inline">\(B\)</span> 同时发生，用 <span class="math inline">\(A \cap B\)</span> 表示；差：事件 <span class="math inline">\(A\)</span> 发生，且事件 <span class="math inline">\(B\)</span> 不发生，用减号 <span class="math inline">\(A - B\)</span> 表示；余：事件 <span class="math inline">\(A\)</span> 和事件 <span class="math inline">\(B\)</span> 只能有一个发生（其实就是非的意思），用 <span class="math inline">\(A = \bar{B}\)</span> 表示。以上运算都满足交换律、结合律和分配律，还满足德摩根定律（De Morgan's laws），也称作反演律，没有什么神奇的，表达式是：<span class="math inline">\(\overline{A \cap B} = \bar{A} \cup \bar{B}\)</span>。</p><h2 id="概率">概率</h2><p>确定事件的概率，常用的方法有频率方法、古典方法、几何方法。</p><p>频率方法，是观察事件 <span class="math inline">\(A\)</span> 大量重复实验，记录 <span class="math inline">\(A\)</span> 出现的次数，也称作 <strong>频数</strong>，频数与总实验数之比记作 <span class="math inline">\(A\)</span> 的概率，即 <span class="math inline">\(p(A) = \frac{n(A)}{N}\)</span>，实践表明随着实验次数的增加，该比值将稳定在真实值附近，但是受到实验次数不能无限进行的限制，该方法只能得到近似的估计值。</p><p>古典方法，是在经验的基础上，通过逻辑分析总结得到概率的计算表达式，基本的计算方法是利用事件 <span class="math inline">\(A\)</span> 在样本空间中所占有的个数与样本空间中所有样本点的个数之比计算 <span class="math inline">\(A\)</span> 的概率，也就是 <span class="math inline">\(P(A) = \frac{N_A}{N}\)</span>。</p><p>几何方法，利用几何工具的辅助，假设样本空间内各处是等可能的，有一个事件的可能充满某一子区域，然后就用这个子区域占整体区域的大小来表示这个区域代表的事件 <span class="math inline">\(A\)</span> 的概率，写成 <span class="math inline">\(P(A) = \frac{S_A}{S}\)</span>。</p><p>此外，还有根据个人的经验和主观判断来定义事件发生的概率，俗话说就是凭感觉，当然这不能是玄学的范畴。</p><p>当然，概率有一些性质和计算规则。比如，<span class="math inline">\(P(\varnothing) = 0\)</span>，不可能事件的概率为 0，<span class="math inline">\(P(\Omega) = 1\)</span>，<span class="math inline">\(\Omega\)</span> 是整个样本空间，必然事件的概率为 1。</p><p>概率是可加的，例如若干个 <strong>互不相容 </strong>的事件 <span class="math inline">\(A_1, A_2, A_3, \cdots, A_n\)</span> 的概率之和为 <span class="math display">\[P(A_{\sum}) = \sum_{i} P(A_i) \]</span></p><p>如果对于一般事件，那么就要使用加法公式了 <span class="math display">\[P(A \cup B) = P(A) + P(B) - P(A \cap B) \]</span> 可以想象两个重叠了一部分的矩形，要求重叠的面积，那么对两个矩形面积求和后还要减去它们中间重叠的部分，这就是加法公式的意义。于是，自然的有一个不等式推论，即 <span class="math inline">\(P(A \cup B) \leq P(A) + P(B)\)</span>。</p><h1 id="条件概率和独立性">条件概率和独立性</h1><h2 id="条件概率">条件概率</h2><p><strong>条件概率 </strong>的定义是在某一个事件 <span class="math inline">\(B\)</span> 发生的情况下，求另一个事件 <span class="math inline">\(A\)</span> 发生的概率，记作 <span class="math inline">\(P(A|B)\)</span>，定义为 <span class="math display">\[P(A|B) = \frac{P(AB)}{P(B)} \]</span> 其中重要的一点是 <span class="math inline">\(P(B) &gt; 0\)</span>。 ## 乘法公式 <strong>乘法公式 </strong>可以直接从上式推导出，若 <span class="math inline">\(P(B) &gt; 0\)</span>，则 <span class="math display">\[P(AB) = P(B)P(A|B) \]</span> 在机器学习中，更为常见的是乘法公式的一般式子，设 <span class="math inline">\(P(A_1 A_2 A_3 \cdots A_{n-1}) &gt; 0\)</span>，则有 <span class="math display">\[P(A_1 \cdots A_n) = P(A_1) P(A_2|A_1) P(A_3|A_1 A_2) \cdots P(A_n| A_1 \cdots A_{n-1}) \]</span></p><h2 id="全概率公式">全概率公式</h2><p><strong>全概率公式 </strong>定义为 <span class="math display">\[P(A) = \sum_{i=1}^{n}{P(B_i)P(A|B_i)} \]</span> 其中，<span class="math inline">\(B_1, B_2 \cdots B_n\)</span> 表示对一个样本空间的分割，也就是 <span class="math inline">\(B_1, B_2 \cdots B_n\)</span> 是互不相容的。</p><h2 id="贝叶斯公式">贝叶斯公式</h2><p><strong>贝叶斯公式 </strong>应该算是概率论中最重要的公式了，它可以帮助我们借助容易计算的概率去计算一些不那么容易计算的概率。其可以记作 <span class="math display">\[P(B|A) = \frac{P(A|B)P(B)}{P(A)} \]</span> 更一般的表达式为 <span class="math display">\[p(B_i|A) = \frac{P(A|B_i)P(B_i)}{\sum_{j}{P(B_j)P(A|B_j)}} \]</span> 其中 <span class="math inline">\(i, j = 1, 2 \cdots n\)</span>，<span class="math inline">\(B_1, B_2 \cdots B_n\)</span> 表示对一个样本空间的分割。</p><h2 id="独立性">独立性</h2><p><strong>独立性 </strong>表示两个事件之间不是相互影响的，就称作这两个事件是 <strong>相互独立 </strong>的，如果有多个事件，当多个事件之间两两相互独立，才能称这些事件相互独立。如果两个事件是相互独立的，那么他们同时发生的概率可以直接由各自发生的概率相乘得到，记作 <span class="math inline">\(P(AB)=P(A)P(B)\)</span>。</p><h2 id="马尔可夫性质">马尔可夫性质</h2><p><strong>马尔可夫性质 </strong>也是机器学习中常用到的基本概念之一，它表示，一个随机过程的状态只跟当前状态有关，而跟过往的所有的状态都无关。用数学公式表达为 <span class="math display">\[P(B_{n+1}|B_1, B_2 \cdots B_n) = P(B_{n+1} | B_n) \]</span></p><h1 id="概率分布">概率分布</h1><p>随机变量有 <strong>连续随机变量 </strong>和<strong>离散随机变量 </strong>之分，那么相应的，用随机变量的 <strong>分布函数 </strong>来描述随机变量在所有取值上的概率，也有 <strong>连续分布 </strong>和<strong>离散分布 </strong>之分。一般来说，分布函数表示的是概率，在整个自变量域内是单调的，而且还是非减的。针对连续的情况，更普遍的是用 <strong>概率密度 </strong>函数来描述概率的分布情况，对于概率密度的积分才得到概率本身（类比密度和质量的关系）。概率密度函数是非负的，而且在整个定义域上的积分之和一定等于 1。在接下来的章节中，如果不加以特殊的说明，那么都是针对连续的情况 而言。</p><h2 id="常用的分布">常用的分布</h2><p>在机器学习中，一些复杂的分布会使用一些简单的同时性质比较好的分布组合而成，因此，我们需要了解一些常用的简单的分布，其中最常见的应当是高斯分布了。</p><h3 id="高斯分布">高斯分布</h3><p><strong>高斯分布 </strong>又叫做 <strong>正太分布</strong>，记作 <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>，也可以写成 <span class="math inline">\(p(x) \sim N(\mu, \sigma^2)\)</span>，其中 <span class="math inline">\(X\)</span> 表示随机变量，<span class="math inline">\(p(x)\)</span> 表示概率密度函数，其公式为 <span class="math display">\[p(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp{\frac{-(x - \mu)^2}{2\sigma^2}} \]</span> 其中，<span class="math inline">\(\mu\)</span> 表示均值，<span class="math inline">\(\sigma\)</span> 为方差。<strong>标准高斯分布 </strong>的参数为 <span class="math inline">\(\mu = 0\)</span>，<span class="math inline">\(\sigma = 1\)</span>。高斯分布很好的性质，比如任意高斯分布可以由一个标准的高斯分布得到，记作 <span class="math inline">\(p(x) \sim N(0, 1)\)</span>，这个性质可是使得随机采样可以参与梯度下降的优化算法，是随机优化算法的基本用法之一。高斯分布的 68.27% 的样本在平均值左右的一个标准差的范围内，95.45% 的样本在平均值左右的两个标准差的范围内，而 99.73% 的样本都分布在平均值左右的三个标准差的范围内。高斯分布能够对现实中很多问题进行建模，当问题非常复杂的时候，还能够使用不同的高斯分布组合成复杂的 <strong>混合高斯分布 </strong>来描述，其中每个组成部分之间的概率表示可以为均匀分布或者自定义的其他分布。</p><h3 id="其他常用分布">其他常用分布</h3><p>其他常见的分布有 <strong>均匀分布</strong>，概率密度函数的数学表达为 <span class="math display">\[p(x) = \left\{\begin{aligned} &amp;\frac{1}{b-a} \qquad a \leq x \leq b \\ &amp;0 \qquad \qquad other \end{aligned} \right. \]</span> 其他的还是有泊松分布，几何分布等等。</p><h2 id="概率分布的数学特征">概率分布的数学特征</h2><h3 id="期望">期望</h3><p><strong>期望 </strong>的定义是随机分布的加权平均值，可以写作 <span class="math display">\[E(X) = \mathbb{E}_{p(x)}(\cdot) = \int_{-\infty}^{＋\infty}xp(x) \ dx \]</span> 其中 <span class="math inline">\(\cdot\)</span> 表示常数，也就是求的是本身的期望，如果替换成函数 <span class="math inline">\(g(x)\)</span> 则表示在这个函数下的期望，在积分中需要乘上这个函数的值，比如 <span class="math display">\[\mathbb{E}_{p(x)}(g(x)) = \int_{-\infty}^{+\infty}g(x)p(x) \ dx \]</span> 严格来说，<span class="math inline">\(g(x)\)</span> 还需要满足处处可导、单调等一些好的数学性质，这里不做专门展开了，证明也不详细介绍了。如果这个积分不收敛，那么可以说期望不存在。</p><h3 id="方差">方差</h3><p><strong>方差 </strong>表示的是随机分布偏离均值的程度，或者说是波动的大小。可以写作 <span class="math display">\[Var(X) = \int_{-\infty}^{+\infty}(x - \mathbb{E}_{p(x)})^2p(x) \ dx \]</span> 方差有一个更加实用的性质是 <span class="math display">\[Var(X) = E(X^2) - (E(X)^2) \]</span> “平方的期望减去期望的平方”，可以在各种场合快速计算方差。</p><h3 id="多维随机分布">多维随机分布</h3><p>多维随机分布简单来说就是样本空间的维度扩升到了高维，于是随机变量成为了 <strong>联合随机变量 </strong>密度函数成为了 <strong>联合分布函数 </strong>，表示每一维的随机事件一起发生的概率，在连续分布中，使用<strong> 联合密度函数 </strong>刻画，一般来说使用向量的方式记录，如 <span class="math inline">\(p(\boldsymbol{x})\)</span>，加粗的 <span class="math inline">\(\boldsymbol{x}\)</span> 表示向量。通过联合密度函数，求联合分布函数，当然也是使用多重积分进行的。最常见的有二维联合分布，<span class="math inline">\(p(\boldsymbol{x},\boldsymbol{y})\)</span>，在计算机视觉里面很常用，比如使用二维高斯分布作滤波器进行滤波操作等等。如果固定其他维度，对于某一个维度进行积分，那么表示这个 <strong>边缘分布</strong>，主要关注的就是对于某一个随机变量的分布。</p><h3 id="协方差">协方差</h3><p>在二维随机分布中还有一个重要的概念，<strong>协方差</strong>。协方差表示两个分量之间的相关性，定义如下 <span class="math display">\[Cov(X,Y) = E[(X - E(X))(Y - E(Y))] \]</span> 如果为 0，则表示不相关，大于 0，是正相关，小于 0，是负相关。由协方差还可以引出 <strong>相关系数</strong>，定义如下 <span class="math display">\[Corr(X,Y) = \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}} \]</span> 当然，在刻画两个变量的相关性的时候，还有更加丰富的方法，比如信息论中的互信息等，在下面会讲到。</p><h2 id="jensen- 不等式">Jensen 不等式</h2><p>正在写...</p></div><div class="popular-posts-header">相关推荐</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="\Zlog\IIC\" rel="bookmark">Invariant Information Clustering (IIC) 无监督的图像分类和分割</a></div><div class="popular-posts-excerpt"><p></p><h1 id="引言">引言</h1><p>2019 年，牛津大学提出了一种无监督，不依赖标签的聚类方法：Invariant Information Clustering (IIC)，即根据给定数据对（Data Pair）之间的互信息（Mutual Information, MI）提供端到端的、无标签的、无监督的方式训练神经网络，使其能够直接输出类别标签，从而实现聚类。</p><p></p></div></li></ul><div class="reward-container"><div>请我喝杯咖啡吧。</div><button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'>打赏</button><div id="qr" style="display:none"><div style="display:inline-block"><img src="/Zlog/images/weixin.jpg" alt="zrzfh 微信支付"><p>微信支付</p></div><div style="display:inline-block"><img src="/Zlog/images/zhifubao.jpg" alt="zrzfh 支付宝"><p>支付宝</p></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/Zlog/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a> <a href="/Zlog/tags/%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7/" rel="tag"># 不确定性</a> <a href="/Zlog/tags/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/" rel="tag"># 模式识别</a> <a href="/Zlog/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" rel="tag"># 概率论</a></div><div class="post-nav"><div class="post-nav-item"><a href="/Zlog/slepc/" rel="prev" title="Ubuntu 自定义路径下安装 SLEPc/PETSc 以及 BLAS/LAPACK 科学计算库"><i class="fa fa-chevron-left"></i> Ubuntu 自定义路径下安装 SLEPc/PETSc 以及 BLAS/LAPACK 科学计算库</a></div><div class="post-nav-item"></div></div></footer></article></div><div class="comments" id="gitalk-container"></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BC%95%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">引言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E4%B8%8E%E6%A6%82%E7%8E%87"><span class="nav-number">2.</span> <span class="nav-text">随机变量与概率</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E4%BA%8B%E4%BB%B6%E5%92%8C%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F"><span class="nav-number">2.1.</span> <span class="nav-text">随机事件和随机变量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E7%8E%87"><span class="nav-number">2.2.</span> <span class="nav-text">概率</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87%E5%92%8C%E7%8B%AC%E7%AB%8B%E6%80%A7"><span class="nav-number">3.</span> <span class="nav-text">条件概率和独立性</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87"><span class="nav-number">3.1.</span> <span class="nav-text">条件概率</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%A8%E6%A6%82%E7%8E%87%E5%85%AC%E5%BC%8F"><span class="nav-number">3.2.</span> <span class="nav-text">全概率公式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F"><span class="nav-number">3.3.</span> <span class="nav-text">贝叶斯公式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%8B%AC%E7%AB%8B%E6%80%A7"><span class="nav-number">3.4.</span> <span class="nav-text">独立性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%80%A7%E8%B4%A8"><span class="nav-number">3.5.</span> <span class="nav-text">马尔可夫性质</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83"><span class="nav-number">4.</span> <span class="nav-text">概率分布</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8%E7%9A%84%E5%88%86%E5%B8%83"><span class="nav-number">4.1.</span> <span class="nav-text">常用的分布</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83"><span class="nav-number">4.1.1.</span> <span class="nav-text">高斯分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E5%B8%B8%E7%94%A8%E5%88%86%E5%B8%83"><span class="nav-number">4.1.2.</span> <span class="nav-text">其他常用分布</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83%E7%9A%84%E6%95%B0%E5%AD%A6%E7%89%B9%E5%BE%81"><span class="nav-number">4.2.</span> <span class="nav-text">概率分布的数学特征</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%9F%E6%9C%9B"><span class="nav-number">4.2.1.</span> <span class="nav-text">期望</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E5%B7%AE"><span class="nav-number">4.2.2.</span> <span class="nav-text">方差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E7%BB%B4%E9%9A%8F%E6%9C%BA%E5%88%86%E5%B8%83"><span class="nav-number">4.2.3.</span> <span class="nav-text">多维随机分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%8F%E6%96%B9%E5%B7%AE"><span class="nav-number">4.2.4.</span> <span class="nav-text">协方差</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#jensen-%20%E4%B8%8D%E7%AD%89%E5%BC%8F"><span class="nav-number">4.3.</span> <span class="nav-text">Jensen 不等式</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="zrzfh" src="/Zlog/images/avatar.jpg"><p class="site-author-name" itemprop="name">zrzfh</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/Zlog/archives/"><span class="site-state-item-count">4</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/Zlog/categories/"><span class="site-state-item-count">3</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/Zlog/tags/"><span class="site-state-item-count">14</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><span class="exturl" data-url="bWFpbHRvOnpob3V6aGVuaHVpXzY4QG91dGxvb2suY29t" title="E-Mail → mailto:zhouzhenhui_68@outlook.com"><i class="fa fa-envelope fa-fw"></i></span></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2023</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">zrzfh</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span title="站点总字数">9k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">34 分钟</span></div><div class="busuanzi-count"><script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-divider">|</span> <span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/Zlog/lib/anime.min.js"></script><script src="/Zlog/lib/pjax/pjax.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script><script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script><script src="/Zlog/lib/velocity/velocity.min.js"></script><script src="/Zlog/lib/velocity/velocity.ui.min.js"></script><script src="/Zlog/js/utils.js"></script><script src="/Zlog/js/motion.js"></script><script src="/Zlog/js/schemes/muse.js"></script><script src="/Zlog/js/next-boot.js"></script><script src="/Zlog/js/bookmark.js"></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});</script><script src="/Zlog/js/local-search.js"></script><div id="pjax"><script>if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }</script><script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script><script>window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://zrzfh.github.io/Zlog/probability/',]
      });
      });</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '945e4cf328a1869624aa',
      clientSecret: 'd7bb1069bb0b644617a97d281490f2e3c67d120e',
      repo        : 'comments',
      owner       : 'zrzfh',
      admin       : ['zrzfh'],
      id          : '6be26966a6a489f0803aa5be9d8d6292',
        language: 'zh-CN',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});</script></div></body></html>