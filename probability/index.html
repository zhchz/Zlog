<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0"><link rel="icon" type="image/png" sizes="32x32" href="/Zlog/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/Zlog/images/favicon.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="google-site-verification" content="JWKXKBgMA-PwT6UVFtoe_WnlTH8jJAuNK1XVhHY_2mM"><link rel="stylesheet" href="/Zlog/css/main.css"><link rel="stylesheet" href="/Zlog/lib/font-awesome/css/all.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"zrzfh.github.io",root:"/Zlog/",scheme:"Mist",version:"7.8.0",exturl:"fasle",sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!0,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!0,pangu:!0,comments:{style:"tabs",active:"gitalk",storage:!0,lazyload:!0,nav:null,activeClass:"gitalk"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="引言 不确定性可以说是在用数学工具对世界进行建模和认知的时候不可回避的问题，也是机器学习中的一个最基本的概念。于是，我们需要对这种不确定性进行量化和计算，而概率论能够为此提供一个合理的框架。所以，概率论是机器学习中最重要的基础理论之一，这也是本文要详细梳理概率论的基础知识的出发点。如果再辅之以其他工具如决策论、线性代数、微积分等，还能够在让我们构建世界观的同时，有切实可用的方法论。"><meta property="og:type" content="article"><meta property="og:title" content="机器学习基础（一）概率论"><meta property="og:url" content="https://zrzfh.github.io/Zlog/probability/index.html"><meta property="og:site_name" content="Zlog"><meta property="og:description" content="引言 不确定性可以说是在用数学工具对世界进行建模和认知的时候不可回避的问题，也是机器学习中的一个最基本的概念。于是，我们需要对这种不确定性进行量化和计算，而概率论能够为此提供一个合理的框架。所以，概率论是机器学习中最重要的基础理论之一，这也是本文要详细梳理概率论的基础知识的出发点。如果再辅之以其他工具如决策论、线性代数、微积分等，还能够在让我们构建世界观的同时，有切实可用的方法论。"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2023-04-03T08:06:15.000Z"><meta property="article:modified_time" content="2023-05-22T07:56:52.136Z"><meta property="article:author" content="zrzfh"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="不确定性"><meta property="article:tag" content="模式识别"><meta property="article:tag" content="概率论"><meta name="twitter:card" content="summary"><link rel="canonical" href="https://zrzfh.github.io/Zlog/probability/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>机器学习基础（一）概率论 | Zlog</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><link rel="alternate" href="/Zlog/atom.xml" title="Zlog" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/Zlog/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">Zlog</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/Zlog/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/Zlog/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/Zlog/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/Zlog/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-about"><a href="/Zlog/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><a role="button" class="book-mark-link book-mark-link-fixed"></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://zrzfh.github.io/Zlog/probability/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/Zlog/images/avatar.jpg"><meta itemprop="name" content="zrzfh"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Zlog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">机器学习基础（一）概率论</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2023-04-03 16:06:15" itemprop="dateCreated datePublished" datetime="2023-04-03T16:06:15+08:00">2023-04-03</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2023-05-22 15:56:52" itemprop="dateModified" datetime="2023-05-22T15:56:52+08:00">2023-05-22</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/Zlog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a> </span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>6.6k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>24 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="引言">引言</h1><p>不确定性可以说是在用数学工具对世界进行建模和认知的时候不可回避的问题，也是机器学习中的一个最基本的概念。于是，我们需要对这种不确定性进行量化和计算，而概率论能够为此提供一个合理的框架。所以，概率论是机器学习中最重要的基础理论之一，这也是本文要详细梳理概率论的基础知识的出发点。如果再辅之以其他工具如决策论、线性代数、微积分等，还能够在让我们构建世界观的同时，有切实可用的方法论。</p><span id="more"></span><p>本文会尽可能全面的列出机器学习中涉及到概率论的重要的基础的公理、定理、推理、定义、算法等，给出重要的公式、流程，但不会涉及证明和详细推导过程，作为梳理和备用，适用于入门和需要快速了解相关原理的情形，渴望更深层次的理解和探讨，建议系统的阅读相关资料书籍。</p><h1 id="随机变量与概率">随机变量与概率</h1><p>首先，我们简单的回顾一下随机事件、随机变量、概率等基本的概念。</p><h2 id="随机事件和随机变量">随机事件和随机变量</h2><p>对于不总是出现同一个结果的现象，我们称之为 <strong>随机现象 </strong>，随机现象所有的可能结果组成了<strong> 样本空间 </strong>，而样本空间中的一个或者几个点集被称为<strong> 随机事件</strong>，通常使用 <span class="math inline">\(A, B, C, ...\)</span> 等大写字母表示。</p><p>于是，用于表示随机事件的结果的数学变量，被称作 <strong>随机变量</strong>，在一般数学表达上使用 <span class="math inline">\(X, Y, Z\)</span> 这样的大写字母。随机变量可以用来表示随机事件或者随机事件的组合，于是涉及到了随机事件之间的关系和运算法则。</p><p>总的来说，随机事件的关系包括：<strong>包含关系 </strong>，<strong> 相等关系 </strong>，<strong> 互不相容</strong>。包含关系是指，事件 <span class="math inline">\(A\)</span> 是事件 <span class="math inline">\(B\)</span> 的子集，<span class="math inline">\(A\)</span> 的发生必然导致 <span class="math inline">\(B\)</span> 的发生，用 <span class="math inline">\(A \subset B\)</span> 表示。相等关系是指，事件 <span class="math inline">\(A\)</span> 和事件 <span class="math inline">\(B\)</span> 相互包含，用 <span class="math inline">\(A = B\)</span> 表示 。互不相容表示，事件 <span class="math inline">\(A\)</span> 和事件 <span class="math inline">\(B\)</span> 没有交集，<span class="math inline">\(A\)</span> 和 <span class="math inline">\(B\)</span> 不可能同时发生，用 <span class="math inline">\(A \cap B = \varnothing\)</span> 表示。</p><p>随机事件的运算法则包含 <strong>并</strong>，<strong>交 </strong>，<strong> 差</strong>，<strong>余</strong>。并：事件 <span class="math inline">\(A\)</span> 或者事件 <span class="math inline">\(B\)</span> 至少有一个发生，用 <span class="math inline">\(A \cup B\)</span> 表示；交：事件 <span class="math inline">\(A\)</span> 和事件 <span class="math inline">\(B\)</span> 同时发生，用 <span class="math inline">\(A \cap B\)</span> 表示；差：事件 <span class="math inline">\(A\)</span> 发生，且事件 <span class="math inline">\(B\)</span> 不发生，用减号 <span class="math inline">\(A - B\)</span> 表示；余：事件 <span class="math inline">\(A\)</span> 和事件 <span class="math inline">\(B\)</span> 只能有一个发生（其实就是非的意思），用 <span class="math inline">\(A = \bar{B}\)</span> 表示。以上运算都满足交换律、结合律和分配律，还满足德摩根定律（De Morgan's laws），也称作反演律，没有什么神奇的，表达式是：<span class="math inline">\(\overline{A \cap B} = \bar{A} \cup \bar{B}\)</span>。</p><h2 id="概率">概率</h2><p>确定事件的概率，常用的方法有频率方法、古典方法、几何方法。</p><p>频率方法，是观察事件 <span class="math inline">\(A\)</span> 大量重复实验，记录 <span class="math inline">\(A\)</span> 出现的次数，也称作 <strong>频数</strong>，频数与总实验数之比记作 <span class="math inline">\(A\)</span> 的概率，即 <span class="math inline">\(p(A) = \frac{n(A)}{N}\)</span>，实践表明随着实验次数的增加，该比值将稳定在真实值附近，但是受到实验次数不能无限进行的限制，该方法只能得到近似的估计值。</p><p>古典方法，是在经验的基础上，通过逻辑分析总结得到概率的计算表达式，基本的计算方法是利用事件 <span class="math inline">\(A\)</span> 在样本空间中所占有的个数与样本空间中所有样本点的个数之比计算 <span class="math inline">\(A\)</span> 的概率，也就是 <span class="math inline">\(P(A) = \frac{N_A}{N}\)</span>。</p><p>几何方法，利用几何工具的辅助，假设样本空间内各处是等可能的，有一个事件的可能充满某一子区域，然后就用这个子区域占整体区域的大小来表示这个区域代表的事件 <span class="math inline">\(A\)</span> 的概率，写成 <span class="math inline">\(P(A) = \frac{S_A}{S}\)</span>。</p><p>此外，还有根据个人的经验和主观判断来定义事件发生的概率，俗话说就是凭感觉，当然这不能是玄学的范畴。</p><p>当然，概率有一些性质和计算规则。比如，<span class="math inline">\(P(\varnothing) = 0\)</span>，不可能事件的概率为 0，<span class="math inline">\(P(\Omega) = 1\)</span>，<span class="math inline">\(\Omega\)</span> 是整个样本空间，必然事件的概率为 1。</p><p>概率是可加的，例如若干个 <strong>互不相容 </strong>的事件 <span class="math inline">\(A_1, A_2, A_3, \cdots, A_n\)</span> 的概率之和为 <span class="math display">\[P(A_{\sum}) = \sum_{i} P(A_i) \]</span></p><p>如果对于一般事件，那么就要使用加法公式了 <span class="math display">\[P(A \cup B) = P(A) + P(B) - P(A \cap B) \]</span> 可以想象两个重叠了一部分的矩形，要求重叠的面积，那么对两个矩形面积求和后还要减去它们中间重叠的部分，这就是加法公式的意义。于是，自然的有一个不等式推论，即 <span class="math inline">\(P(A \cup B) \leq P(A) + P(B)\)</span>。</p><h1 id="条件概率和独立性">条件概率和独立性</h1><h2 id="条件概率">条件概率</h2><p><strong>条件概率 </strong>的定义是在某一个事件 <span class="math inline">\(B\)</span> 发生的情况下，求另一个事件 <span class="math inline">\(A\)</span> 发生的概率，记作 <span class="math inline">\(P(A|B)\)</span>，定义为 <span class="math display">\[P(A|B) = \frac{P(AB)}{P(B)} \]</span> 其中重要的一点是 <span class="math inline">\(P(B) &gt; 0\)</span>。 ## 乘法公式 <strong>乘法公式 </strong>可以直接从上式推导出，若 <span class="math inline">\(P(B) &gt; 0\)</span>，则 <span class="math display">\[P(AB) = P(B)P(A|B) \]</span> 在机器学习中，更为常见的是乘法公式的一般式子，设 <span class="math inline">\(P(A_1 A_2 A_3 \cdots A_{n-1}) &gt; 0\)</span>，则有 <span class="math display">\[P(A_1 \cdots A_n) = P(A_1) P(A_2|A_1) P(A_3|A_1 A_2) \cdots P(A_n| A_1 \cdots A_{n-1}) \]</span></p><h2 id="全概率公式">全概率公式</h2><p><strong>全概率公式 </strong>定义为 <span class="math display">\[P(A) = \sum_{i=1}^{n}{P(B_i)P(A|B_i)} \]</span> 其中，<span class="math inline">\(B_1, B_2 \cdots B_n\)</span> 表示对一个样本空间的分割，也就是 <span class="math inline">\(B_1, B_2 \cdots B_n\)</span> 是互不相容的。</p><h2 id="贝叶斯公式">贝叶斯公式</h2><p><strong>贝叶斯公式 </strong>应该算是概率论中最重要的公式了，它可以帮助我们借助容易计算的概率去计算一些不那么容易计算的概率。其可以记作 <span class="math display">\[P(B|A) = \frac{P(A|B)P(B)}{P(A)} \]</span> 更一般的表达式为 <span class="math display">\[p(B_i|A) = \frac{P(A|B_i)P(B_i)}{\sum_{j}{P(B_j)P(A|B_j)}} \]</span> 其中 <span class="math inline">\(i, j = 1, 2 \cdots n\)</span>，<span class="math inline">\(B_1, B_2 \cdots B_n\)</span> 表示对一个样本空间的分割。</p><h2 id="独立性">独立性</h2><p><strong>独立性 </strong>表示两个事件之间不是相互影响的，就称作这两个事件是 <strong>相互独立 </strong>的，如果有多个事件，当多个事件之间两两相互独立，才能称这些事件相互独立。如果两个事件是相互独立的，那么他们同时发生的概率可以直接由各自发生的概率相乘得到，记作 <span class="math inline">\(P(AB)=P(A)P(B)\)</span>。</p><h2 id="马尔可夫性质">马尔可夫性质</h2><p><strong>马尔可夫性质 </strong>也是机器学习中常用到的基本概念之一，它表示，一个随机过程的状态只跟当前状态有关，而跟过往的所有的状态都无关。用数学公式表达为 <span class="math display">\[P(B_{n+1}|B_1, B_2 \cdots B_n) = P(B_{n+1} | B_n) \]</span></p><h1 id="概率分布">概率分布</h1><p>随机变量有 <strong>连续随机变量 </strong>和<strong>离散随机变量 </strong>之分，那么相应的，用随机变量的 <strong>分布函数 </strong>来描述随机变量在所有取值上的概率，也有 <strong>连续分布 </strong>和<strong>离散分布 </strong>之分。一般来说，分布函数表示的是概率，在整个自变量域内是单调的，而且还是非减的。针对连续的情况，更普遍的是用 <strong>概率密度 </strong>函数来描述概率的分布情况，对于概率密度的积分才得到概率本身（类比密度和质量的关系）。概率密度函数是非负的，而且在整个定义域上的积分之和一定等于 1。在接下来的章节中，如果不加以特殊的说明，那么都是针对连续的情况 而言。</p><h2 id="常用的分布">常用的分布</h2><p>在机器学习中，一些复杂的分布会使用一些简单的同时性质比较好的分布组合而成，因此，我们需要了解一些常用的简单的分布，其中最常见的应当是高斯分布了。</p><h3 id="高斯分布">高斯分布</h3><p><strong>高斯分布 </strong>又叫做 <strong>正太分布</strong>，记作 <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>，也可以写成 <span class="math inline">\(p(x) \sim N(\mu, \sigma^2)\)</span>，其中 <span class="math inline">\(X\)</span> 表示随机变量，<span class="math inline">\(p(x)\)</span> 表示概率密度函数，其公式为 <span class="math display">\[p(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp{\frac{-(x - \mu)^2}{2\sigma^2}} \]</span> 其中，<span class="math inline">\(\mu\)</span> 表示均值，<span class="math inline">\(\sigma^2\)</span> 为方差。<strong>标准高斯分布 </strong>的参数为 <span class="math inline">\(\mu = 0\)</span>，<span class="math inline">\(\sigma = 1\)</span>。高斯分布很好的性质，比如任意高斯分布可以由一个标准的高斯分布得到，记作 <span class="math inline">\(p(x) \sim N(0, 1)\)</span>，这个性质可是使得随机采样可以参与梯度下降的优化算法，是随机优化算法的基本用法之一。高斯分布的 68.27% 的样本在平均值左右的一个标准差的范围内，95.45% 的样本在平均值左右的两个标准差的范围内，而 99.73% 的样本都分布在平均值左右的三个标准差的范围内。高斯分布能够对现实中很多问题进行建模，当问题非常复杂的时候，还能够使用不同的高斯分布组合成复杂的 <strong>混合高斯分布 </strong>来描述，其中每个组成部分之间的概率表示可以为均匀分布或者自定义的其他分布。</p><h3 id="其他常用分布">其他常用分布</h3><p>其他常见的分布有 <strong>均匀分布</strong>，概率密度函数的数学表达为 <span class="math display">\[p(x) = \left\{\begin{aligned} &amp;\frac{1}{b-a} \qquad a \leq x \leq b \\ &amp;0 \qquad \qquad other \end{aligned} \right. \]</span></p><h3 id="复杂的分布">复杂的分布</h3><p>事实上，现实里的数据都是服从一些非常复杂的分布，对它们需要进行建模分析，可以使用 <strong>高斯混合模型</strong>，用于近似描述。高斯混合模型是多个高斯分布的线性集合，可以表示 <span class="math display">\[p(x) = \sum_{i=1}^{k}\pi_i N(\mu_i, \sigma_i^2) \]</span> 其中，<span class="math inline">\(\pi_i\)</span> 表示第 <span class="math inline">\(i\)</span> 个高斯分布的系数（或者也称作这个高斯分布出现的概率），而且满足 <span class="math inline">\(\sum_{i}\pi_i = 1\)</span>，<span class="math inline">\(\mu_i, \sigma_i^2\)</span> 表示第 <span class="math inline">\(i\)</span> 个高斯分布的均值和方差。</p><h2 id="概率分布的数学特征">概率分布的数学特征</h2><h3 id="期望">期望</h3><p><strong>期望 </strong>的定义是随机分布的加权平均值，可以写作 <span class="math display">\[E(X) = \mathbb{E}_{p(x)}[\cdot] = \int_{-\infty}^{＋\infty}xp(x) \ dx \]</span> 其中 <span class="math inline">\(\cdot\)</span> 表示常数，也就是求的是本身的期望，如果替换成函数 <span class="math inline">\(g(x)\)</span> 则表示在这个函数下的期望，在积分中需要乘上这个函数的值，比如 <span class="math display">\[\mathbb{E}_{p(x)}[g(x)] = \int_{-\infty}^{+\infty}g(x)p(x) \ dx \]</span> 严格来说，<span class="math inline">\(g(x)\)</span> 还需要满足处处可导、单调等一些好的数学性质，这里不做专门展开了，证明也不详细介绍了。如果这个积分不收敛，那么可以说期望不存在。</p><h3 id="方差">方差</h3><p><strong>方差 </strong>表示的是随机分布偏离均值的程度，或者说是波动的大小。可以写作 <span class="math display">\[Var(X) = \int_{-\infty}^{+\infty}(x - \mathbb{E}_{p(x)}[\cdot])^2p(x) \ dx \]</span> 方差有一个更加实用的性质是 <span class="math display">\[Var(X) = E(X^2) - (E(X)^2) \]</span> “平方的期望减去期望的平方”，可以在各种场合快速计算方差。</p><h3 id="多维随机分布">多维随机分布</h3><p>多维随机分布简单来说就是样本空间的维度扩升到了高维，于是随机变量成为了 <strong>联合随机变量 </strong>密度函数成为了 <strong>联合分布函数 </strong>，表示每一维的随机事件一起发生的概率，在连续分布中，使用<strong> 联合密度函数 </strong>刻画，一般来说使用向量的方式记录，如 <span class="math inline">\(p(\boldsymbol{x})\)</span>，加粗的 <span class="math inline">\(\boldsymbol{x}\)</span> 表示向量。通过联合密度函数，求联合分布函数，当然也是使用多重积分进行的。最常见的有二维联合分布，<span class="math inline">\(p(\boldsymbol{x},\boldsymbol{y})\)</span>，在计算机视觉里面很常用，比如使用二维高斯分布作滤波器进行滤波操作等等。如果固定其他维度，对于某一个维度进行积分，那么表示这个 <strong>边缘分布</strong>，主要关注的就是对于某一个随机变量的分布。</p><h3 id="协方差">协方差</h3><p>在二维随机分布中还有一个重要的概念，<strong>协方差</strong>。协方差表示两个分量之间的相关性，定义如下 <span class="math display">\[Cov(X,Y) = E[(X - E(X))(Y - E(Y))] \]</span> 如果为 0，则表示不相关，大于 0，是正相关，小于 0，是负相关。由协方差还可以引出 <strong>相关系数</strong>，定义如下 <span class="math display">\[Corr(X,Y) = \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}} \]</span> 当然，在刻画两个变量的相关性的时候，还有更加丰富的方法，比如信息论中的互信息等，在下面会讲到。</p><h2 id="jensen- 不等式">Jensen 不等式</h2><p><strong>Jensen 不等式 </strong>是一个常用的不等式，假设给定一个凸函数，如标准的二次函数，在上面作任意两点之间的割线，那么所得到的割线一定在函数的上方，这就是 Jensen 不等式的几何意义。它的定义是，对一平均做凸函数变换，会小于等于先做凸函数变换再平均，于是在使用概率密度函数时，我们可以得到 <span class="math display">\[\begin{split} \phi (\int_{-\infty}^{+\infty} g(x) p(x) \ dx) &amp;\leq \int_{-\infty}^{+\infty} \phi(g(x)) p(x) \ dx \\ \phi (\mathbb{E}_{p(x)}[g(x)]) &amp; \leq \mathbb{E}_{p(x)}[\phi(g(x))p(x)] \end{split} \]</span></p><h2 id="特征函数">特征函数</h2><p>如果考虑将密度函数作傅里叶变换，可以得到 <span class="math display">\[\phi(t) = \int_{-\infty}^{+\infty} e^{itx} \ dx = \mathbb{E}_{p(x)}[e^{itx}] \]</span> 其中 <span class="math inline">\(i = \sqrt{-1}\)</span> 为虚数单位。于是，傅里叶变化就是关于函数 <span class="math inline">\(g(x,t) = e^{itx}\)</span> 的均值。密度函数的傅里叶变换有着许多用处，比如能把独立随机变量和的分布的卷积运算 (积分运算) 转换成乘法运算，能把求分布的各阶原点矩 (积分运算) 转换成微分运算，能把随机变量序列的极限分布转换成一般的函数极限问题，通常也把密度函数的傅里叶变换称作为概率密度分布的 <strong>特征函数</strong>。</p><p>特征函数有许多好的数学性质：</p><p>1、 <span class="math inline">\(|\phi(t)| \leq \phi(0) = 1\)</span><br>2、 <span class="math inline">\(\phi(-t)\)</span> 与 <span class="math inline">\(\phi(t)\)</span> 互为共轭，为 <span class="math inline">\(\phi(-t) = \overline{\phi(t)}\)</span><br>3、 概率分布的和的特征函数为特征函数的积，为 <span class="math inline">\(\phi_{p(x) + p(y)}(t) = \phi_{p(x)}(t) \cdot \phi_{p(y)}(t)\)</span><br>4、 设 <span class="math inline">\(\mathbb{E}[x^{l}]\)</span> 存在，则其特征函数 <span class="math inline">\(\phi(t)\)</span> <span class="math inline">\(l\)</span> 次可导，且 <span class="math inline">\(\phi^{k}(0) = i^{k} \mathbb{E}[x^{l}]\)</span><br>5、 特征函数在 <span class="math inline">\((-\infty, +\infty)\)</span> 一致连续<br>6、 特征函数是非负的<br>7、 特征函数是唯一的</p><h1 id="参数估计">参数估计</h1><p>参数估计主要用于估计随机分布中的未知参数、未知参数的函数或者未知的特征数或函数等，在机器学习中，最广泛的应用主要在于估计设定的随机分布的参数，例如最大似然估计、最大化期望算法、贝叶斯估计等等。这里，我们主要回顾一下这些经典的算法。</p><h2 id="最大似然估计">最大似然估计</h2><p><strong>最大似然估计 </strong>叫做 Maximum Likelihood Estimation，简称 MLE，是根据已有的数据进行验证，使得所给定的模型和所估计的模型参数在所给的数据上得到最大的符合。如果我们已有模型，那么根据模型写出一致数据所发生的概率，成为似然函数，然后似然函数对参数求导，使之为 0，那么就可以解出所的参数。用数学表示，记　<span class="math inline">\(p(x|\theta)\)</span>，<span class="math inline">\(\theta \in \Theta\)</span>, 其中 <span class="math inline">\(\theta\)</span> 表示一个或多个未知参数，且其所有可能的取值空间用 <span class="math inline">\(\Theta\)</span> 表示。现在有一些列的观测数据 <span class="math inline">\(x_1, x_2, \cdots x_n\)</span>，那么似然函数可以写成 <span class="math display">\[\begin{split} L(\theta;x_1, x_2, \cdots x_n) &amp;= p(x_1, x_2, \cdots x_n | \theta) \\ &amp;= \prod \limits_{i=1}^{n}p(x_i|\theta) \end{split} \]</span> 简记 <span class="math inline">\(L(\theta)\)</span>，最大似然估计可以写成 <span class="math display">\[\begin{split} \hat{\theta} &amp;= \arg\max_{\theta \in \Theta} L(\theta) \\ &amp;= \arg\max_{\theta \in \Theta} \log L(\theta) \\ &amp;= \arg\max_{\theta \in \Theta} \log \prod \limits_{i=1}^{n}p(x_i|\theta) \\ &amp;= \arg\max_{\theta \in \Theta} \sum_{i=1}^{n} \log p(x_i|\theta) \end{split} \]</span></p><h2 id="最大化数学期望">最大化数学期望</h2><p><strong>最大化数学期望 </strong>叫做 Expectation Maximization，简称 EM。EM 算法是优化算法的一种，它的概率模型通常依赖于无法观测的隐变量，因此事实上我们是不能够直接求解方程的，也无法使用梯度下降的方式，于是它采用一种优化迭代的思想逼近最优。</p><p>例如，我们需要估计学校中男生、女生的身高分布模型，通过随机抽样得到一些样本，但由于粗心导致男生和女生的样本混合在一起，并且无法区分了，也没有任何关于学校中男生、女生的人数分布模型的信息可供参考。</p><p>如果能够区分男生、女生的样本，或者有先验的男生、女生的人数分布模型，那么我们完全可以使用最大似然估计，假设男女生身高分布都是高斯分布，那么对于男生的样本或者属于男生样本的概率，我们可以根据高斯分布写出所有样本都出现的概率，然后构造似然函数，似然函数就是以参数为未知变量，而样本为已知量，那么最后将似然函数对参数求导，并令倒数为 0，即可求得最大似然估计的参数。</p><p>但现在的问题是，我们无法区分男生、女生的样本，也就是说这里引入了一个无法求得的隐藏变量，它决定了男生还是女生的样本。于是这里使用 EM 算法。在这个例子中，首先初始化男生或者女生样本的身高分布的参数，然后估计每个样本更可能是属于男生还是女生，在当前估计下，使用最大似然估计当前数据下的参数，然后不断迭代直到参数稳定。</p><p>上面通过例子比较感性的了解了 EM 的算法思想，那么接下来通过数学工具严谨的推导一遍。假设现有一系列数学样本 <span class="math inline">\(\boldsymbol{x} = \{x_1, x_2, \cdots x_n \}\)</span>，且有一系列对应的隐藏变量 <span class="math inline">\(\boldsymbol{z} = \{z_1, z_2, \cdots z_n \}\)</span> 来给出样本的一些特定信息，于是有 <span class="math inline">\(p(x) = \int_{z} p(x, z)\)</span>，设参数用 <span class="math inline">\(\theta\)</span> 表示，现在我们写出这些样本的似然函数并取对数（将乘法拆成加法） <span class="math display">\[L(\theta | \boldsymbol{x}, \boldsymbol{z}) = \int_{i=1}^{n}{\log \int_{z_i} p(x_i, z_i|\theta)} \]</span></p><p>如果我们将 <span class="math inline">\(Q_i(z_i)\)</span> 看成是 <span class="math inline">\(z_i\)</span> 的分布，但这个分布目前是不知道的，于是我们可以对上式的右边部分进行变化得到 <span class="math display">\[\begin{split} \int_{i=1}^{n}\log \int_{z_i} p(x_i, z_i|\theta) &amp;= \int_{i=1}^{n}\log \int_{z_i} Q_i(z_i) \frac{p(x_i, z_i|\theta)}{Q_i(z_i)} \\ &amp; \geq \int_{i=1}^{n} \int_{z_i} Q_i(z_i) \log \frac{p(x_i, z_i|\theta)}{Q_i(z_i)} \end{split} \]</span> 上式中的不等号是使用 Jensen 不等式的结果。所以，最后通过不断提高下界的方式，也就是不断求上式中不等号的右边的式子的最大值，并以此来逼近左侧的最大值。直观的理解方式是，先固定 <span class="math inline">\(\theta\)</span>，优化 <span class="math inline">\(Q_i(z_i)\)</span>，使得下界达到最大，然后固定 <span class="math inline">\(Q_i(z_i)\)</span>，优化 <span class="math inline">\(\theta\)</span> 求似然函数的最大值，如此反复就能逼近最大的值，也对应了 EM 算法的 E 步骤和 M 步骤。其中，不等式取等号的时候，是在 <span class="math inline">\(\frac{p(x_i, z_i|\theta)}{Q_i(z_i)}\)</span> 这一项为常数的时候，此时 <span class="math inline">\(\log\)</span> 在积分符号外侧或者内侧都不影响积分运算（求均值运算）。于是有 <span class="math display">\[p(x_i, z_i|\theta) = CQ_i(z_i) \]</span> 对两侧同时积分（关于 <span class="math inline">\(z_i\)</span>） <span class="math display">\[\int_{z_i}p(x_i, z_i|\theta) = C\int_{z_i}Q_i(z_i) \]</span> 其实，在给出 <span class="math inline">\(Q_i(z_i)\)</span> 的时候，暗含了一个隐藏条件 <span class="math inline">\(\int_{z_i}Q_i(z_i) = 1\)</span>，这样的构造能使得问题简化，于是 <span class="math inline">\(\int_{z_i}p(x_i, z_i|\theta) = C\)</span> 也是常数。最后发现 <span class="math display">\[\begin{split} Q_i(z_i) &amp;= \frac{p(x_i, z_i|\theta)}{\int_{z_i}p(x_i, z_i|\theta)} \\ &amp;= \frac{p(x_i, z_i|\theta)}{p(x_i|\theta)} \\ &amp;= p(z_i|x_i, \theta) \end{split} \]</span> 是一个后验概率，所以计算 <span class="math inline">\(Q_i(z_i)\)</span> 的时候就是计算后验概率。</p><h2 id="最大后验估计">最大后验估计</h2><p>最大似然估计是频率学派的代表，而与之相对的贝叶斯学派的代表是 <strong>最大后验估计</strong>，叫作 Maximum a Posteriori estimation，简称 MAP。相比于最大似然估计，最大后验估计加入了参数的先验分布的假设，然后通过数据对齐进行调整。设 <span class="math inline">\(p(x|\theta)\)</span>，<span class="math inline">\(\theta \in \Theta\)</span>, 其中 <span class="math inline">\(\theta\)</span> 表示一个或多个未知参数，且其所有可能的取值空间用 <span class="math inline">\(\Theta\)</span> 表示，同样，现在有一些列的观测数据 <span class="math inline">\(x_1, x_2, \cdots x_n\)</span>。那么后验概率 <span class="math inline">\(p(\theta|x)\)</span> 可以用贝叶斯公式求解， <span class="math display">\[p(\theta|x) = \frac{p(x|\theta) p(\theta)}{p(x)} \propto p(x|\theta) p(\theta) \]</span> 于是有 <span class="math display">\[\begin{split} \arg\max_{\theta \in \Theta} p(\theta|x) &amp;= \arg\max_{\theta \in \Theta} p(x|\theta)p(\theta) \\ &amp;= \arg\max_{\theta \in \Theta} p(x_1, x_2, \cdots x_n|\theta)p(\theta) \\ &amp;= \arg\max_{\theta \in \Theta} \left( \prod \limits_{i=1}^{n}p(x_i|\theta) \right ) p(\theta) \\ &amp;= \arg\max_{\theta \in \Theta} \left(\sum_{i=1}^{n} \log p(x_i|\theta) \right ) + \log p(\theta) \end{split} \]</span> 然后，仍然可以使用对参数 <span class="math inline">\(\theta\)</span> 求导，并且置为 0，使用梯度下降的方式求解最终结果。</p><p>最大似然估计，认为事件的出现是一种确定的方式，跟谁去观察是没有关系的，所以直接对事件进行建模，从而直接从使得这些事件的出现更加合理的基础上求解参数，在有大量的数据的情况下能够较为准确的得到模型参数。而最大后验估计，则认为这些事件的出现是不确定的，跟谁去观察是有关系的，不同的观察将出现不同的结果，也就是说不同的观察将导致不同的模型或者参数，于是可以说模型或者参数本身服从一定的分布。如果给出这些模型或者参数的先验分布的假设，后续可以通过观察数据进行校验，从而使的估计的模型或者参数能够趋于接近真实的情况。当给出的先验模型能够比较符合实际时，能够得到比较好的结果，当观测数据量越来越大时，会削弱先验模型的主导。值得一提的是，如果假设先验模型或者参数服从均匀分布，那么最大后验估计和最大似然估计就是一样的了。</p><h1 id="熵与 -kl- 散度">熵与 KL 散度</h1><p>在机器学习中，经常使用的基本概念还有熵、KL 散度（Kullback–Leibler Divergence）等。接下来，我们再简单回顾一下这些基本概念和一些常用的数学技巧。</p><h2 id="熵">熵</h2><p><strong>熵 </strong>通常是用来测量随机变量的不确定度的。设变量 <span class="math inline">\(x\)</span> 服从分布 <span class="math inline">\(p(x)\)</span>, 记作 <span class="math inline">\(x \sim p(x)\)</span>，那么随机变量 <span class="math inline">\(X\)</span> 的熵 <span class="math inline">\(H(X)\)</span> 定义为 <span class="math display">\[H(X) = - \int_{x \in \mathcal{X}} p(x) \ log \ p(x) \]</span></p><p>当然，如果有一对随机变量服从联合分布，记作 <span class="math inline">\(x, y \sim ~ p(x,y)\)</span>，那么 <strong>联合熵</strong> <span class="math inline">\(H(X,Y)\)</span> 显然可以写作 <span class="math display">\[H(X, Y) = - \int_{x \in \mathcal{X}} \int_{y \in \mathcal{Y}}p(x, y) \ log \ p(x,y) \]</span></p><p>这一对随机变量的 <strong>条件熵</strong> <span class="math inline">\(H(Y|X)\)</span> 可以写成 <span class="math display">\[\begin{split} H(Y|X) &amp;= \int_{x \in \mathcal{X}} p(x) \ H(Y|X=x) \\ &amp;= - \int_{x \in \mathcal{X}} p(x) \int_{y \in \mathcal{Y}} p(y|x) \ log \ p(y|x) \\ &amp;= - \int_{x \in \mathcal{X}} \int_{y \in \mathcal{Y}} p(x, y) \ log \ p(y|x) \end{split} \]</span></p><p>其中，有个常用的定理链式法则 <span class="math display">\[\begin{split} H(X, Y) &amp;= H(X) + H(Y|X) \\ H(X, Y|Z) &amp;= H(X|Z) + H(Y|X, Z) \end{split} \]</span></p><h2 id="kl- 散度">KL 散度</h2><p><strong>KL 散度</strong>（Kullback–Leibler Divergence, KLD）也称作相对熵，是用来测量两个分布之间的“距离”（或者称之为相似程度）。两个分布 <span class="math inline">\(p(x), q(x)\)</span> 之间的 KL 散度定义为 <span class="math display">\[\begin{equation} D_{KL}(p(x)||q(x)) = \int_{x \in \mathcal{X}} p(x)log\frac{p(x)}{q(x)} \end{equation} \]</span></p><p>KL 散度的几个小性质：</p><p>1、<span class="math inline">\(D_{KL}(p(x)||q(x)) \geq 0\)</span>，</p><p>2、<span class="math inline">\(D_{KL}(p(x)||q(x)) = 0\)</span>，当且仅当 <span class="math inline">\(p(x) = q(x)\)</span>，</p><p>3、<span class="math inline">\(D_{KL}(p(x)||q(x)) \neq D_{KL}(q(x)||p(x))\)</span>。</p><h2 id="互信息与自信息">互信息与自信息</h2><p>熵和 KL 散度还联系到信息论中的自信息与互信息的概念，这里也简单介绍一下。</p><h3 id="互信息">互信息</h3><p><strong>互信息 </strong>定义为，给定联合分布 <span class="math inline">\(p(x, y)\)</span> 和边缘分布 <span class="math inline">\(p(x), p(y)\)</span>， <span class="math display">\[\begin{equation} \begin{split} I(X; Y) &amp;= \int_{x \in \mathcal{X}} \int_{y \in \mathcal{Y}} p(x, y) \ log \ \frac{p(x,y)}{p(x)p(y)} \\ &amp;= D_{KL}(p(x,y)||p(x)p(y)) \end{split} \label{midef} \end{equation} \]</span> <span class="math inline">\(I(X;Y)\)</span> 可以理解成联合分布 <span class="math inline">\(p(x,y)\)</span> 与边缘分布乘积 <span class="math inline">\(p(x)p(y)\)</span> 之间的 KL 散度。通过观察，可以将公式 <span class="math inline">\(\eqref{midef}\)</span> 进一步变化得到 <span class="math display">\[\begin{equation} \begin{split} I(X; Y) &amp;= \int_{x \in \mathcal{X}} \int_{y \in \mathcal{Y}} p(x, y) \ log \ \frac{p(x,y)}{p(x)p(y)} \\ &amp;= \int_{x \in \mathcal{X}} \int_{y \in \mathcal{Y}} p(x, y) \ log \ \frac{p(x|y)}{p(x)} \\ &amp;= - \int_{x \in \mathcal{X}} \int_{y \in \mathcal{Y}} p(x, y) \ log \ p(x) + \int_{x \in \mathcal{X}} \int_{y \in \mathcal{Y}} p(x, y) \ log \ p(x|y) \\ &amp;= - \int_{x \in \mathcal{X}} p(x) \ log \ p(x) - \left(- \int_{x \in \mathcal{X}} \int_{y \in \mathcal{Y}} p(x, y) \ log \ p(x|y) \right) \\ &amp;= H(X) - H(X|Y) \end{split} \label{mitra} \end{equation} \]</span> 这就是 KL 散度和熵之间的联系。</p><h3 id="自信息">自信息</h3><p>如果把公式 <span class="math inline">\(\eqref{mitra}\)</span> 中的 <span class="math inline">\(Y\)</span> 随机变量替换成 <span class="math inline">\(X\)</span> 随机变量，就可以得到 <span class="math display">\[\begin{equation} I(X;X) = H(X) - H(X|X) = H(X) \label{sidef} \end{equation} \]</span> 这被定义为自信息，从公式 <span class="math inline">\(\eqref{sidef}\)</span> 来看，熵和自信息在某种程度上是一致的。</p></div><div class="popular-posts-header">相关推荐</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="\Zlog\STEGO\" rel="bookmark">Self-supervised Transformer with Energy-based Graph Optimization（STEGO）无监督语义分割</a></div><div class="popular-posts-excerpt"><p></p><h1 id="引言">引言</h1><p>无监督的语义分割是图像处中最具挑战的领域之一，在一些例如医学、生物或者深空探索等领域，对于图像的真实标注是无法获取的、不是完全可信的、又或者是压根的人都没见过的，即使真实标注是可知可控的，标注所需要的人力成本也是巨大的。于是，如果一个模型有无监督语义分割的能力，能在一定程度上给出基于模型自身“理解”的有意义的分割（这里的“理解”并不一定表示模型是强智能的、和人一样的理解，而是模型对于分割或者分类按照某种足够可信的方式组织完成的、或者说可以被人理解和认可的），就显得尤为重要。2022年，STEGO 就给出了这样一种尝试，在无监督语义分割上达到了 SOTA 的性能。</p><p></p></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="\Zlog\IIC\" rel="bookmark">Invariant Information Clustering (IIC) 无监督的图像分类和分割</a></div><div class="popular-posts-excerpt"><p></p><h1 id="引言">引言</h1><p>2019年，牛津大学提出了一种无监督，不依赖标签的聚类方法：Invariant Information Clustering (IIC)，即根据给定数据对（Data Pair）之间的互信息（Mutual Information, MI）提供端到端的、无标签的、无监督的方式训练神经网络，使其能够直接输出类别标签，从而实现聚类。</p><p></p></div></li></ul><div class="reward-container"><div>请我喝杯咖啡吧。</div><button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'>打赏</button><div id="qr" style="display:none"><div style="display:inline-block"><img src="/Zlog/images/weixin.jpg" alt="zrzfh 微信支付"><p>微信支付</p></div><div style="display:inline-block"><img src="/Zlog/images/zhifubao.jpg" alt="zrzfh 支付宝"><p>支付宝</p></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/Zlog/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a> <a href="/Zlog/tags/%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7/" rel="tag"># 不确定性</a> <a href="/Zlog/tags/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/" rel="tag"># 模式识别</a> <a href="/Zlog/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" rel="tag"># 概率论</a></div><div class="post-nav"><div class="post-nav-item"><a href="/Zlog/slepc/" rel="prev" title="Ubuntu 自定义路径下安装 SLEPc/PETSc 以及 BLAS/LAPACK 科学计算库"><i class="fa fa-chevron-left"></i> Ubuntu 自定义路径下安装 SLEPc/PETSc 以及 BLAS/LAPACK 科学计算库</a></div><div class="post-nav-item"><a href="/Zlog/inference/" rel="next" title="机器学习基础（二）近似推断">机器学习基础（二）近似推断 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="gitalk-container"></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BC%95%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">引言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E4%B8%8E%E6%A6%82%E7%8E%87"><span class="nav-number">2.</span> <span class="nav-text">随机变量与概率</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E4%BA%8B%E4%BB%B6%E5%92%8C%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F"><span class="nav-number">2.1.</span> <span class="nav-text">随机事件和随机变量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E7%8E%87"><span class="nav-number">2.2.</span> <span class="nav-text">概率</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87%E5%92%8C%E7%8B%AC%E7%AB%8B%E6%80%A7"><span class="nav-number">3.</span> <span class="nav-text">条件概率和独立性</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87"><span class="nav-number">3.1.</span> <span class="nav-text">条件概率</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%A8%E6%A6%82%E7%8E%87%E5%85%AC%E5%BC%8F"><span class="nav-number">3.2.</span> <span class="nav-text">全概率公式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F"><span class="nav-number">3.3.</span> <span class="nav-text">贝叶斯公式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%8B%AC%E7%AB%8B%E6%80%A7"><span class="nav-number">3.4.</span> <span class="nav-text">独立性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%80%A7%E8%B4%A8"><span class="nav-number">3.5.</span> <span class="nav-text">马尔可夫性质</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83"><span class="nav-number">4.</span> <span class="nav-text">概率分布</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8%E7%9A%84%E5%88%86%E5%B8%83"><span class="nav-number">4.1.</span> <span class="nav-text">常用的分布</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83"><span class="nav-number">4.1.1.</span> <span class="nav-text">高斯分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E5%B8%B8%E7%94%A8%E5%88%86%E5%B8%83"><span class="nav-number">4.1.2.</span> <span class="nav-text">其他常用分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%8D%E6%9D%82%E7%9A%84%E5%88%86%E5%B8%83"><span class="nav-number">4.1.3.</span> <span class="nav-text">复杂的分布</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83%E7%9A%84%E6%95%B0%E5%AD%A6%E7%89%B9%E5%BE%81"><span class="nav-number">4.2.</span> <span class="nav-text">概率分布的数学特征</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%9F%E6%9C%9B"><span class="nav-number">4.2.1.</span> <span class="nav-text">期望</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E5%B7%AE"><span class="nav-number">4.2.2.</span> <span class="nav-text">方差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E7%BB%B4%E9%9A%8F%E6%9C%BA%E5%88%86%E5%B8%83"><span class="nav-number">4.2.3.</span> <span class="nav-text">多维随机分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%8F%E6%96%B9%E5%B7%AE"><span class="nav-number">4.2.4.</span> <span class="nav-text">协方差</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#jensen-%20%E4%B8%8D%E7%AD%89%E5%BC%8F"><span class="nav-number">4.3.</span> <span class="nav-text">Jensen 不等式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%87%BD%E6%95%B0"><span class="nav-number">4.4.</span> <span class="nav-text">特征函数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1"><span class="nav-number">5.</span> <span class="nav-text">参数估计</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1"><span class="nav-number">5.1.</span> <span class="nav-text">最大似然估计</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%80%E5%A4%A7%E5%8C%96%E6%95%B0%E5%AD%A6%E6%9C%9F%E6%9C%9B"><span class="nav-number">5.2.</span> <span class="nav-text">最大化数学期望</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1"><span class="nav-number">5.3.</span> <span class="nav-text">最大后验估计</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%86%B5%E4%B8%8E%20-kl-%20%E6%95%A3%E5%BA%A6"><span class="nav-number">6.</span> <span class="nav-text">熵与 KL 散度</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%86%B5"><span class="nav-number">6.1.</span> <span class="nav-text">熵</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kl-%20%E6%95%A3%E5%BA%A6"><span class="nav-number">6.2.</span> <span class="nav-text">KL 散度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%92%E4%BF%A1%E6%81%AF%E4%B8%8E%E8%87%AA%E4%BF%A1%E6%81%AF"><span class="nav-number">6.3.</span> <span class="nav-text">互信息与自信息</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%92%E4%BF%A1%E6%81%AF"><span class="nav-number">6.3.1.</span> <span class="nav-text">互信息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E4%BF%A1%E6%81%AF"><span class="nav-number">6.3.2.</span> <span class="nav-text">自信息</span></a></li></ol></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="zrzfh" src="/Zlog/images/avatar.jpg"><p class="site-author-name" itemprop="name">zrzfh</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/Zlog/archives/"><span class="site-state-item-count">6</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/Zlog/categories/"><span class="site-state-item-count">3</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/Zlog/tags/"><span class="site-state-item-count">14</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><span class="exturl" data-url="bWFpbHRvOnpob3V6aGVuaHVpXzY4QG91dGxvb2suY29t" title="E-Mail → mailto:zhouzhenhui_68@outlook.com"><i class="fa fa-envelope fa-fw"></i></span></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2023</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">zrzfh</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span title="站点总字数">14k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">50 分钟</span></div><div class="busuanzi-count"><script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-divider">|</span> <span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/Zlog/lib/anime.min.js"></script><script src="/Zlog/lib/pjax/pjax.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script><script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script><script src="/Zlog/lib/velocity/velocity.min.js"></script><script src="/Zlog/lib/velocity/velocity.ui.min.js"></script><script src="/Zlog/js/utils.js"></script><script src="/Zlog/js/motion.js"></script><script src="/Zlog/js/schemes/muse.js"></script><script src="/Zlog/js/next-boot.js"></script><script src="/Zlog/js/bookmark.js"></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});</script><script src="/Zlog/js/local-search.js"></script><div id="pjax"><script>if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }</script><script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script><script>window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://zrzfh.github.io/Zlog/probability/',]
      });
      });</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '945e4cf328a1869624aa',
      clientSecret: 'd7bb1069bb0b644617a97d281490f2e3c67d120e',
      repo        : 'comments',
      owner       : 'zrzfh',
      admin       : ['zrzfh'],
      id          : '6be26966a6a489f0803aa5be9d8d6292',
        language: 'zh-CN',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});</script></div></body></html>