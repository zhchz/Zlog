<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0"><link rel="icon" type="image/png" sizes="32x32" href="/Zlog/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/Zlog/images/favicon.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="google-site-verification" content="JWKXKBgMA-PwT6UVFtoe_WnlTH8jJAuNK1XVhHY_2mM"><link rel="stylesheet" href="/Zlog/css/main.css"><link rel="stylesheet" href="/Zlog/lib/font-awesome/css/all.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"zrzfh.github.io",root:"/Zlog/",scheme:"Mist",version:"7.8.0",exturl:"fasle",sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!0,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!0,pangu:!0,comments:{style:"tabs",active:"gitalk",storage:!0,lazyload:!0,nav:null,activeClass:"gitalk"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="引言 2019 年，牛津大学提出了一种无监督，不依赖标签的聚类方法：Invariant Information Clustering (IIC)，即根据给定数据对（Data Pair）之间的互信息（Mutual Information, MI）提供端到端的、无标签的、无监督的方式训练神经网络，使其能够直接输出类别标签，从而实现聚类。"><meta property="og:type" content="article"><meta property="og:title" content="Invariant Information Clustering (IIC) 无监督的图像分类和分割"><meta property="og:url" content="https://zrzfh.github.io/Zlog/IIC/index.html"><meta property="og:site_name" content="Zlog"><meta property="og:description" content="引言 2019 年，牛津大学提出了一种无监督，不依赖标签的聚类方法：Invariant Information Clustering (IIC)，即根据给定数据对（Data Pair）之间的互信息（Mutual Information, MI）提供端到端的、无标签的、无监督的方式训练神经网络，使其能够直接输出类别标签，从而实现聚类。"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2023-03-10T02:07:52.000Z"><meta property="article:modified_time" content="2023-03-15T06:56:46.250Z"><meta property="article:author" content="zrzfh"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="无监督学习"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="图像分割"><meta property="article:tag" content="图像分类"><meta name="twitter:card" content="summary"><link rel="canonical" href="https://zrzfh.github.io/Zlog/IIC/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>Invariant Information Clustering (IIC) 无监督的图像分类和分割 | Zlog</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><link rel="alternate" href="/Zlog/atom.xml" title="Zlog" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/Zlog/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">Zlog</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/Zlog/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/Zlog/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/Zlog/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/Zlog/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-about"><a href="/Zlog/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><a role="button" class="book-mark-link book-mark-link-fixed"></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://zrzfh.github.io/Zlog/IIC/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/Zlog/images/avatar.jpg"><meta itemprop="name" content="zrzfh"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Zlog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Invariant Information Clustering (IIC) 无监督的图像分类和分割</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2023-03-10 10:07:52" itemprop="dateCreated datePublished" datetime="2023-03-10T10:07:52+08:00">2023-03-10</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2023-03-15 14:56:46" itemprop="dateModified" datetime="2023-03-15T14:56:46+08:00">2023-03-15</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/Zlog/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">论文笔记</span></a> </span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>2.6k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>9 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="引言">引言</h1><p>2019 年，牛津大学提出了一种无监督，不依赖标签的聚类方法：Invariant Information Clustering (IIC)，即根据给定数据对（Data Pair）之间的互信息（Mutual Information, MI）提供端到端的、无标签的、无监督的方式训练神经网络，使其能够直接输出类别标签，从而实现聚类。</p><span id="more"></span><h1 id="原理方法">原理方法</h1><p>IIC 建立在一个基本想法上，将单条数据经过简单并且不涉及材料本身畸变的仿射变化后，在变化前后的数据对中，同一个物体的描述应该（或者说必须）是一致的，当然，这里不包括物体上具体的细节。如果定义一个函数映射，将数据映射到某个空间（比如类别），那么只要通过优化，使得同一数据对在映射空间的各自输出能够保持一致就行了。</p><h2 id="问题思路">问题思路</h2><p>设 <span class="math inline">\(x, x^{\prime} \in \mathcal {X}\)</span> 是一对数据对，服从分布 <span class="math inline">\(p (x,x^\prime)\)</span>, 即 <span class="math inline">\(x, x^{\prime} \sim p (x, x^{\prime})\)</span>，IIC 的目的是学习得到一个映射 <span class="math inline">\(\Phi: \mathcal {X} \rightarrow \mathcal {Y}\)</span>，能够保持 <span class="math inline">\(x, x^{\prime}\)</span> 两者之间共性（如含有的同一个物体），而忽略特定的细节（如物体上的具体细节信息），其中 <span class="math inline">\(\mathcal {X}\)</span> 是样本集合，<span class="math inline">\(\mathcal {Y} = \{1, \dots,C\}\)</span> 是有限集合。用数学语言描述，<span class="math inline">\(\Phi\)</span> 可以通过 <strong>​最大化数据对在映射空间的互信息（MI）​</strong> 得到，用公式表示为 <span class="math display">\[ \mathop {max}\limits_{\Phi} \ I (\Phi (x), \Phi (x^{\prime})) \]</span> 通常来说，<span class="math inline">\(I (x)\)</span> 中已经包含了熵（Entropy），可以避免模型退化（Degeneracy），可以认为，含有熵的模型不至于直接收缩到某一个点上。此处，<span class="math inline">\(\Phi\)</span> 可以是一个神经网络，论文中使用 “瓶颈（Bottleneck）” 结构，可用于忽略特定的细节。</p><h2 id="数学定义">数学定义</h2><p>互信息（MI）是信息论中的基本概念，与之相对的还有自信息（Self-Information）。而熵（Entropy）和相对熵（Relative Entropy, RE）是与之相关且不可或缺的重要基础概念之一。在使用之前，我们来简单回顾一下这些基础概念。</p><p>为了方便，下文约定：用 <span class="math inline">\(p (x)\)</span> 专门指代随机变量 <span class="math inline">\(X:\mathcal {X} \rightarrow \mathbb {R}\)</span> 的概率密度函数，<span class="math inline">\(\mathcal {X}\)</span> 为样本空间，且 <span class="math inline">\(x \in \mathcal {X}\)</span>。以此类推，指代同一个随机变量的相关参数或者变量，会使用同一个字母的不同字体表示。例如需要注意到 <span class="math inline">\(p (x)\)</span> 和 <span class="math inline">\(p (y)\)</span> 分别指代两个随机变量 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 的概率密度函数。而且，在本文中对于离散随机变量和连续随机变量不加以区分，公式中都使用离散随机变量表示。如果使用连续随机变量，那么需要把求和符号替换成积分符号。</p><h3 id="熵entropy">熵（Entropy）</h3><p>在信息论中，熵（Entropy）是用来测量随机变量的不确定度的，设变量 <span class="math inline">\(x\)</span> 服从分布 <span class="math inline">\(p (x)\)</span>, 记作 <span class="math inline">\(x \sim p (x)\)</span>，那么随机变量 <span class="math inline">\(X\)</span> 的熵（Entropy）<span class="math inline">\(H (X)\)</span> 定义为 <span class="math display">\[ H (X) = - \sum_{x \in \mathcal {X}} p (x) \ log \ p (x) \]</span></p><p>当然，如果有一对随机变量服从联合分布，记作 <span class="math inline">\(x, y \sim ~ p (x,y)\)</span>，那么联合熵（Joint Entropy） <span class="math inline">\(H (X,Y)\)</span> 显然可以写作 <span class="math display">\[ H (X, Y) = - \sum_{x \in \mathcal {X}} \sum_{y \in \mathcal {Y}} p (x, y) \ log \ p (x,y) \]</span></p><p>这一对随机变量的条件熵（Conditional Entropy）<span class="math inline">\(H (Y|X)\)</span> 可以写成 <span class="math display">\[ \begin {split} H (Y|X) &amp;= \sum_{x \in \mathcal {X}} p (x) \ H (Y|X=x) \\ &amp;= - \sum_{x \in \mathcal {X}} p (x) \sum_{y \in \mathcal {Y}} p (y|x) \ log \ p (y|x) \\ &amp;= - \sum_{x \in \mathcal {X}} \sum_{y \in \mathcal {Y}} p (x, y) \ log \ p (y|x) \end {split} \]</span></p><p>其中，有个常用的定理链式法则（Chain Rule） <span class="math display">\[ \begin {split} H (X, Y) &amp;= H (X) + H (Y|X) \\ H (X, Y|Z) &amp;= H (X|Z) + H (Y|X, Z) \end {split} \]</span></p><h3 id="相对熵re">相对熵（RE）</h3><p>相对熵（RE）是用来测量两个分布之间的 “距离”（或者称之为相似程度），也可称为 KL 散度（Kullback–Leibler Divergence, KLD）。两个分布 <span class="math inline">\(p (x), q (x)\)</span> 之间的 KL 散度（KLD） 定义为 <span class="math display">\[ \begin {equation} D_{KL}(p (x)||q (x)) = \sum_{x \in \mathcal {X}} p (x) log\frac {p (x)}{q (x)} \end {equation} \]</span></p><p>KL 散度（KLD）的几个小性质：</p><p>1、<span class="math inline">\(D_{KL}(p (x)||q (x)) \geq 0\)</span>，</p><p>2、<span class="math inline">\(D_{KL}(p (x)||q (x)) = 0\)</span>，当且仅当 <span class="math inline">\(p (x) = q (x)\)</span>，</p><p>3、<span class="math inline">\(D_{KL}(p (x)||q (x)) \neq D_{KL}(q (x)||p (x))\)</span>。</p><h3 id="互信息mi">互信息（MI）</h3><p>接下来给出互信息（MI）的定义。给定联合分布 <span class="math inline">\(p (x, y)\)</span> 和边缘分布 <span class="math inline">\(p (x), p (y)\)</span>，互信息（MI）<span class="math inline">\(I (X; Y)\)</span> 定义为 <span class="math display">\[ \begin {equation} \begin {split} I (X; Y) &amp;= \sum_{x \in \mathcal {X}} \sum_{y \in \mathcal {Y}} p (x, y) \ log \ \frac {p (x,y)}{p (x) p (y)} \\ &amp;= D_{KL}(p (x,y)||p (x) p (y)) \end {split} \label {midef} \end {equation} \]</span> <span class="math inline">\(I (X;Y)\)</span> 可以理解成联合分布 <span class="math inline">\(p (x,y)\)</span> 与边缘分布乘积 <span class="math inline">\(p (x) p (y)\)</span> 之间的 KL 散度（KLD）。通过观察，可以将公式 <span class="math inline">\(\eqref {midef}\)</span> 进一步变化得到 <span class="math display">\[ \begin {equation} \begin {split} I (X; Y) &amp;= \sum_{x \in \mathcal {X}} \sum_{y \in \mathcal {Y}} p (x, y) \ log \ \frac {p (x,y)}{p (x) p (y)} \\ &amp;= \sum_{x \in \mathcal {X}} \sum_{y \in \mathcal {Y}} p (x, y) \ log \ \frac {p (x|y)}{p (x)} \\ &amp;= - \sum_{x \in \mathcal {X}} \sum_{y \in \mathcal {Y}} p (x, y) \ log \ p (x) + \sum_{x \in \mathcal {X}} \sum_{y \in \mathcal {Y}} p (x, y) \ log \ p (x|y) \\ &amp;= - \sum_{x \in \mathcal {X}} p (x) \ log \ p (x) - \left (- \sum_{x \in \mathcal {X}} \sum_{y \in \mathcal {Y}} p (x, y) \ log \ p (x|y) \right) \\ &amp;= H (X) - H (X|Y) \end {split} \label {mitra} \end {equation} \]</span> 这就是互信息（MI）和熵（Entropy）之间的联系。</p><h3 id="自信息self-information">自信息（Self-Information）</h3><p>如果把公式 <span class="math inline">\(\eqref {mitra}\)</span> 中的 <span class="math inline">\(Y\)</span> 随机变量替换成 <span class="math inline">\(X\)</span> 随机变量，就可以得到 <span class="math display">\[ \begin {equation} I (X;X) = H (X) - H (X|X) = H (X) \label {sidef} \end {equation} \]</span> 这被定义为自信息（Self-Information），从公式 <span class="math inline">\(\eqref {sidef}\)</span> 来看，熵（Entropy）和自信息（Self-Information）在某种程度上是一致的。</p><h2 id="具体方式">具体方式</h2><p>论文考虑使用软聚类（soft clustering），因此将输出 <span class="math inline">\(\Phi (x) \in [0, 1]^C\)</span> 表示成一个离散分布，即 <span class="math inline">\(p (z=c|x) = {\Phi}_c (x)\)</span>，于是一对变量 <span class="math inline">\(x, x^\prime\)</span> 的输出结果 <span class="math inline">\(z, z^\prime\)</span> 可以表示成一个联合分布： <span class="math display">\[ P (z=c, z^\prime=c^\prime|x, x^\prime) = {\Phi}_c (x) \cdot {\Phi}_c (x^\prime) \]</span> 实际上，<span class="math inline">\(z, z^\prime\)</span> 不是独立的，而且还是强相关的。在数据集（实际处理是 batch）上进行边缘化得到一个联合分布 <span class="math inline">\(P\)</span>, <span class="math inline">\(P\)</span> 使用矩阵表示，大小为 <span class="math inline">\([C \times C]\)</span>，于是其中元素可以写成 <span class="math display">\[ \begin {equation} P_{cc^\prime} = \frac {1}{n}\sum_{i=1}^{n}\Phi (x_i)\cdot\Phi ({x_i}^\prime)^T \label {matdef} \end {equation} \]</span> 且一般来说将 <span class="math inline">\(P\)</span> 用 <span class="math inline">\((P+P^T)/2\)</span> 构造成一个对称矩阵。其中，<span class="math inline">\(P_c=P (z=c)\)</span> 和 <span class="math inline">\(P_{c^\prime}=P (z^\prime=c^\prime)\)</span> 能够直接通过矩阵中相应的行或列累加得到。最终，得到互信息（MI）的表达式： <span class="math display">\[ I (z,z^\prime) = I (P) = \sum_{c=1}^{C}\sum_{c^\prime=1}^{C} P_{cc^\prime} \cdot ln \ \frac {P_{cc^\prime}}{P_{c} P_{c^\prime}} \]</span> 作为目标函数（Objective Function）。</p><h3 id="图像聚类">图像聚类</h3><p>在图像聚类中，考虑对采样 <span class="math inline">\(x\)</span> 进行随机的扰动（比如平移、旋转，灰度映射等）变化得到 <span class="math inline">\(x^\prime\)</span>，记作 <span class="math inline">\(x^\prime=gx\)</span>，因此将以上 <span class="math inline">\(x^\prime\)</span> 代替成随机扰动（变换）后的 <span class="math inline">\(gx\)</span> 即可。</p><h3 id="图像分割">图像分割</h3><p>在图像分割中，由于需要对每个像素都进行划分，所以要对图像进行分块（patch），并且要利用块之间的空间位置信息。于是，需要给目标函数（Objective Function）增加局部空间不变性（Local Spatial Invariance）。具体的，如果给定一张 RGB 图像 <span class="math inline">\(x\in \mathbb {R}^{3 \times H \times W}\)</span>, <span class="math inline">\(u \in \Omega = \{1, \ldots, H\} \times \{1, \ldots, W\}\)</span> 表示一个像素位置，<span class="math inline">\(x_u\)</span> 是以 <span class="math inline">\(u\)</span> 为中心的一个块（patch）。假设现存在一个 <span class="math inline">\(t \in T \subset \mathbb {Z}^2\)</span>，使得 <span class="math inline">\(u+t\)</span> 为 <span class="math inline">\(u\)</span> 的领域，那么可以构成一对数据 <span class="math inline">\(\Phi (x_u), \Phi (x_{u+t})\)</span>，而它们可以直接从 <span class="math inline">\(\eqref {matdef}\)</span> 所表示的矩阵中按列读取，因为自始至终都只用了一个神经网络映射 <span class="math inline">\(\Phi\)</span>。</p><p>记 <span class="math inline">\(\Phi (x_u) = \Phi_u (x)\)</span>，那么它对应的另一个数据应为 <span class="math inline">\(\Phi_{g (u)}(gx)\)</span>，因为坐标也因为随机扰动已经变化了。如果以坐标变化前参考，那么可以写成 <span class="math inline">\(\Phi_{g (u)}(gx) = {\big [ g^{-1} \Phi (gx) \big ]}_u\)</span>。因此，<span class="math inline">\(\Phi_u (x)\)</span> 和 <span class="math inline">\({\big [ g^{-1} \Phi (gx) \big ]}_u\)</span> 构成一对数据对，也就是说只要在 <span class="math inline">\(\Phi_u (gx)\)</span> 的基础上再进行一次 <span class="math inline">\(g^{-1}\)</span> 逆变换，就可以构成原 <span class="math inline">\(\Phi_u (x)\)</span> 的数据对。于是，现在给出图像分割的目标函数： <span class="math display">\[ \begin {equation} \begin {split} \frac {1}{|T|}\sum_{t \in T} I (\Phi_u (x);g^{-1}\Phi_u (gx)) &amp;= \frac {1}{|T|}\sum_{t \in T} I (P_t) \\ &amp;= \frac {1}{|T|}\sum_{t \in T} I (\frac {1}{n|G||\Omega|}\sum_{i=1}^{n}\sum_{g \in G}\sum_{u \in \Omega}\Phi_u (x_i) \cdot \big [ g^{-1} \Phi (gx_i) \big ]^{T}_{u+t}) \end {split} \label {sodef} \end {equation} \]</span> 其中，第一个求和符号表示对 <span class="math inline">\(i=1,\ldots,n\)</span> 个图像（实际指得是 batch）进行求和，第二个求和符号表示对于任意的随机变化进行求和，第三个求和符号表示对每张图像中的块进行求和。论文原文中，把最后一个求和部分看成了 Convolution（为什么？）。然后根据最大化目标函数，求解最优的 <span class="math inline">\(\Phi\)</span>。</p><h1 id="总结">总结</h1><p>IIC 最大的优点之一应该就是避免了大量的数据标注问题，能够提供一个无监督的方式对图像进行语义级别的分割。作者很巧妙的使用了随机变换生成的数据对中两者的互信息（MI），似乎更像是一种 “差分” 的方式。其中，作者还提到该模型能够很好避免模型退化。论文的想法非常值得学习，对于不同分布的相似程度的评价以及映射函数，似乎还能有更好的优化，不知道增加一些惩罚项能否进一步增强模型能力，另外使用更强的映射转换模型（transformer）是否也能够进一步优化？</p><p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.06653">https://arxiv.org/abs/1807.06653</a><br>代码地址：<a target="_blank" rel="noopener" href="https://github.com/xu-ji/IIC">https://github.com/xu-ji/IIC</a><br>GitHub 上作者给出的代码用起来很不方便，还是以 python2 环境为主，仅支持 GPU ... 挖个坑，优化一下代码的使用和环境配置...</p></div><div class="popular-posts-header">相关推荐</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="\Zlog\probability\" rel="bookmark">机器学习基础（一）概率论</a></div><div class="popular-posts-excerpt"><p></p><h1 id="引言">引言</h1><p>不确定性可以说是在用数学工具对世界进行建模和认知的时候不可回避的问题，也是机器学习中的一个最基本的概念。于是，我们需要对这种不确定性进行量化和计算，而概率论能够为此提供一个合理的框架。所以，概率论是机器学习中最重要的基础理论之一，这也是本文要详细梳理概率论的基础知识的出发点。如果再辅之以其他工具如决策论、线性代数、微积分等，还能够在让我们构建世界观的同时，有切实可用的方法论。</p><p></p></div></li></ul><div class="reward-container"><div>请我喝杯咖啡吧。</div><button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'>打赏</button><div id="qr" style="display:none"><div style="display:inline-block"><img src="/Zlog/images/weixin.jpg" alt="zrzfh 微信支付"><p>微信支付</p></div><div style="display:inline-block"><img src="/Zlog/images/zhifubao.jpg" alt="zrzfh 支付宝"><p>支付宝</p></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/Zlog/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a> <a href="/Zlog/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" rel="tag"># 无监督学习</a> <a href="/Zlog/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a> <a href="/Zlog/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/" rel="tag"># 图像分割</a> <a href="/Zlog/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/" rel="tag"># 图像分类</a></div><div class="post-nav"><div class="post-nav-item"><a href="/Zlog/%E5%BA%8F/" rel="prev" title="序"><i class="fa fa-chevron-left"></i> 序</a></div><div class="post-nav-item"><a href="/Zlog/slepc/" rel="next" title="Ubuntu 自定义路径下安装 SLEPc/PETSc 以及 BLAS/LAPACK 科学计算库">Ubuntu 自定义路径下安装 SLEPc/PETSc 以及 BLAS/LAPACK 科学计算库 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="gitalk-container"></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BC%95%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">引言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8E%9F%E7%90%86%E6%96%B9%E6%B3%95"><span class="nav-number">2.</span> <span class="nav-text">原理方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%80%9D%E8%B7%AF"><span class="nav-number">2.1.</span> <span class="nav-text">问题思路</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E5%AD%A6%E5%AE%9A%E4%B9%89"><span class="nav-number">2.2.</span> <span class="nav-text">数学定义</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%86%B5entropy"><span class="nav-number">2.2.1.</span> <span class="nav-text">熵（Entropy）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%B8%E5%AF%B9%E7%86%B5re"><span class="nav-number">2.2.2.</span> <span class="nav-text">相对熵（RE）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%92%E4%BF%A1%E6%81%AFmi"><span class="nav-number">2.2.3.</span> <span class="nav-text">互信息（MI）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E4%BF%A1%E6%81%AFself-information"><span class="nav-number">2.2.4.</span> <span class="nav-text">自信息（Self-Information）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B7%E4%BD%93%E6%96%B9%E5%BC%8F"><span class="nav-number">2.3.</span> <span class="nav-text">具体方式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E8%81%9A%E7%B1%BB"><span class="nav-number">2.3.1.</span> <span class="nav-text">图像聚类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2"><span class="nav-number">2.3.2.</span> <span class="nav-text">图像分割</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">3.</span> <span class="nav-text">总结</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="zrzfh" src="/Zlog/images/avatar.jpg"><p class="site-author-name" itemprop="name">zrzfh</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/Zlog/archives/"><span class="site-state-item-count">4</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/Zlog/categories/"><span class="site-state-item-count">3</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/Zlog/tags/"><span class="site-state-item-count">14</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><span class="exturl" data-url="bWFpbHRvOnpob3V6aGVuaHVpXzY4QG91dGxvb2suY29t" title="E-Mail → mailto:zhouzhenhui_68@outlook.com"><i class="fa fa-envelope fa-fw"></i></span></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2023</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">zrzfh</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span title="站点总字数">7k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">26 分钟</span></div><div class="busuanzi-count"><script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-divider">|</span> <span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/Zlog/lib/anime.min.js"></script><script src="/Zlog/lib/pjax/pjax.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script><script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script><script src="/Zlog/lib/velocity/velocity.min.js"></script><script src="/Zlog/lib/velocity/velocity.ui.min.js"></script><script src="/Zlog/js/utils.js"></script><script src="/Zlog/js/motion.js"></script><script src="/Zlog/js/schemes/muse.js"></script><script src="/Zlog/js/next-boot.js"></script><script src="/Zlog/js/bookmark.js"></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});</script><script src="/Zlog/js/local-search.js"></script><div id="pjax"><script>if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }</script><script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script><script>window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://zrzfh.github.io/Zlog/IIC/',]
      });
      });</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '945e4cf328a1869624aa',
      clientSecret: 'd7bb1069bb0b644617a97d281490f2e3c67d120e',
      repo        : 'comments',
      owner       : 'zrzfh',
      admin       : ['zrzfh'],
      id          : 'd2cd870fabc4c1f14071830fdffe5647',
        language: 'zh-CN',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});</script></div></body></html>